{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Crossword Solver Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 1\n",
    "from lxml import html\n",
    "from utils import *\n",
    "import requests\n",
    "import calendar\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "\n",
    "## 2\n",
    "from __future__ import print_function\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import sys\n",
    "\n",
    "## 3\n",
    "\n",
    "## 4\n",
    "import urllib\n",
    "import functools\n",
    "import itertools\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "## 5\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### zero #### \n",
    "(utility function)\n",
    "* IN\n",
    "* OUT\n",
    "\n",
    "#### scrape ####\n",
    "* IN\n",
    "* OUT\n",
    "\n",
    "#### retreive ####\n",
    "* IN\n",
    "    * url (str): URL of the page at www.nytimescrossword.com to scrape\n",
    "* OUT \n",
    "    * clues (dict): props Across and Down are lists of dicts with props location, clue, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zero(n):\n",
    "    if len(str(n)) == 1:\n",
    "        return \"0\"+str(n)\n",
    "    else:\n",
    "        return str(n)\n",
    "\n",
    "def scrape():\n",
    "    collection = []\n",
    "    errors = list()\n",
    "    year = \"2017\"\n",
    "    for i in range(9,10):\n",
    "        day = 25#calendar.monthrange(int(year),i)[1]\n",
    "        for j in range(day,day+1):\n",
    "            #url = \"http://www.nytcrossword.com/\"+year+\"/\"+zero(i)+\"/\"+zero(i)+zero(j)+\"-\"+year[2:]+\"-new-york-times-crossword.html\"\n",
    "            url = \"http://www.nytcrossword.com/\"+year+\"/\"+zero(i)+\"/\"+zero(i)+zero(j)+\"-\"+year[2:]+\"-ny-times-crossword-answers\"\n",
    "            collection.append({\n",
    "                \"date\": year+\"-\"+zero(i)+'-'+zero(j),\n",
    "                \"clues\": retrieve(url)\n",
    "            })\n",
    "\n",
    "    errorfile = open('./errors.txt','w')\n",
    "    errorfile.write('\\n'.join(map(lambda x: x[0]+\";\"+x[1],errors)))\n",
    "    errorfile.close()\n",
    "\n",
    "    datafile = open('./data/data_'+year+'.json','w')\n",
    "    datafile.write(json.dumps(collection,indent=1))\n",
    "    datafile.close()\n",
    "\n",
    "def retrieve(url):\n",
    "    page = requests.get(url)\n",
    "    tree = html.fromstring(page.content)\n",
    "    #raw = [x for x in tree.xpath('//i/text()') if x != '\\n']\n",
    "    raw = [x for x in tree.xpath('//h2[@id=\"all-clues\"]/following-sibling::p[@class=\"no_bottom_margin\"]/em/text()')]\n",
    "    clues = {\n",
    "        \"Across\": list(),\n",
    "        \"Down\": list()\n",
    "    }\n",
    "    direction = \"Across\"\n",
    "    lastLocNum = 0\n",
    "    for d in raw:\n",
    "        try:\n",
    "            dotPos = d.index(\".\")\n",
    "            locNum = d[1:dotPos]\n",
    "            if direction == \"Across\":\n",
    "                if int(locNum) < lastLocNum:\n",
    "                    direction = \"Down\"\n",
    "                else:\n",
    "                    lastLocNum = int(locNum)\n",
    "            location = locNum+\"-\"+direction\n",
    "            clue, answer = re.split('\\s\\:\\s+(?=[A-Z\\‘\\-\\.\\…]+)',d[dotPos+1:])\n",
    "            clues[direction].append({\n",
    "                \"location\": location,\n",
    "                \"clue\": re.sub(u\"(\\u2018|\\u2019|\\u201c|\\u201d)\",\"'\",clue.strip('[ \\-\\.]')),\n",
    "                \"answer\": re.sub('['+string.punctuation+',\\\\t]','',answer)\n",
    "            })\n",
    "        except:\n",
    "            try:\n",
    "                clue, answer = re.split('\\s+(?=[A-Z]{2,})',d[dotPos+1:],1)\n",
    "                clues[direction].append({\n",
    "                    \"location\": location,\n",
    "                    \"clue\": re.sub(u\"(\\u2018|\\u2019|\\u201c|\\u201d)\",\"'\",clue.strip('[ \\-\\.]')),\n",
    "                    \"answer\": re.sub('['+string.punctuation+',\\\\t]','',answer)\n",
    "                })\n",
    "            except:\n",
    "                print((url, d))\n",
    "                errors.append((url, d))\n",
    "    return clues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scrape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saveImage ####\n",
    "* IN\n",
    "* OUT\n",
    "\n",
    "#### parseAndPrintImage ####\n",
    "* IN\n",
    "* OUT\n",
    "\n",
    "#### saveMultipleImages ####\n",
    "* IN\n",
    "* OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def saveImage(url):\n",
    "    page = requests.get(url)\n",
    "    tree = html.fromstring(page.content)\n",
    "    path = '//a[@href=\"#top\"]/img/@src' #'+url+'\n",
    "    imgUrl = tree.xpath(path)[0]\n",
    "    with open('./images/'+url[36:43]+'.png','wb') as file:\n",
    "        img = requests.get(imgUrl,stream=True)\n",
    "        for chunk in img:\n",
    "            file.write(chunk)\n",
    "\n",
    "def parseAndPrintImage(url,nSquares):\n",
    "    imgLoc = './images/'+url[32:39]+'.png'\n",
    "    im = Image.open(imgLoc)\n",
    "    im = Image.composite(im, Image.new('RGB', im.size, 'white'), im)\n",
    "    pps = im.size[0]/nSquares\n",
    "    # im.show()\n",
    "    for i in range(nSquares):\n",
    "        row = list()\n",
    "        for j in range(nSquares):\n",
    "            boxOuter = (j*pps,i*pps,j*pps+pps,i*pps+pps)\n",
    "            tile = im.crop(boxOuter)\n",
    "            #tile.save('./images/tile_'+str(i)+'_'+str(j)+'.jpg')\n",
    "            if not tile.getbbox():\n",
    "                tileText = \" \"\n",
    "            else:\n",
    "                boxInner = (7,9,27,27)\n",
    "                tileInner = tile.crop(boxInner)\n",
    "                tileText = pytesseract.image_to_string(tileInner,config=\"-psm 10 -l eng -c tessedit_char_whitelist=\"+string.ascii_uppercase)\n",
    "                ## this is hacky - find a better way\n",
    "                tileText = \"I\" if tileText == \"\" else tileText\n",
    "            row.append(tileText)\n",
    "        print(\" \".join(row))\n",
    "\n",
    "def saveMultipleImages():\n",
    "    year = \"2017\"\n",
    "    for i in range(9,10):\n",
    "        day = 25#calendar.monthrange(int(year),i)[1]\n",
    "        for j in range(day,day+1):\n",
    "            #url = \"http://www.nytcrossword.com/\"+year+\"/\"+zero(i)+\"/\"+zero(i)+zero(j)+\"-\"+year[2:]+\"-new-york-times-crossword.html\"\n",
    "            url = \"http://www.nytcrossword.com/\"+year+\"/\"+zero(i)+\"/\"+zero(i)+zero(j)+\"-\"+year[2:]+\"-ny-times-crossword-answers\"\n",
    "            saveImage(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "saveMultipleImages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A D D E R   D A B   A F T E R\n",
      "D I A N A   E M U   D R O N E\n",
      "H A N D Y   B I L   H O I T Y\n",
      "O R D   G R U E L   E S T E E\n",
      "C Y Y O U N G   M A R T Y R S\n",
      "      A N A   T O N E        \n",
      "T R E K S   F O O D S T A M P\n",
      "W A V E   H O T S Y   W I R E\n",
      "O P E N H O U S E   D I D I N\n",
      "        O P R Y   A I R      \n",
      "S T D E N I S   S C U L P T S\n",
      "I H O P E   P O W E R   A A H\n",
      "L O V E Y   E G O   N A M B Y\n",
      "T R E E D   E R R   A M B L E\n",
      "S A Y S O   D E N   L A Y E R\n"
     ]
    }
   ],
   "source": [
    "parseAndPrintImage('http://nytcrossword.com/2017/09/0925-17-ny-times-crossword-answers',15)\n",
    "#'http://nytcrossword.com/2017/09/0925-17-ny-times-crossword-answers'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Format Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clue ####\n",
    "* METHODS\n",
    "\n",
    "#### getBlackSquares ####\n",
    "* IN\n",
    "* OUT\n",
    "\n",
    "#### getNewClueWithLength ####\n",
    "* IN\n",
    "* OUT\n",
    "\n",
    "#### placeClues ####\n",
    "* IN\n",
    "* OUT\n",
    "\n",
    "#### mergeWithClues ####\n",
    "* IN\n",
    "* OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Clue(object):\n",
    "\tdef __init__(self,direction,locNum,coords=(0,0),length=0):\n",
    "\t\tself.location = str(locNum)+\"-\"+direction\n",
    "\t\tself.length = length\n",
    "\t\tself.coords = self.getcoords(coords,length,direction)\n",
    "\t\tself.answer = \"\"\n",
    "\tdef getcoords(self,start,length,direction):\n",
    "\t\tcoords = [start]\n",
    "\t\tif direction == \"Across\":\n",
    "\t\t\tfor j in range(1,length):\n",
    "\t\t\t\tcoords.append((start[0],start[1]+j))\n",
    "\t\telse:\n",
    "\t\t\tfor i in range(1,length):\n",
    "\t\t\t\tcoords.append((start[0]+i,start[1]))\n",
    "\t\treturn coords\n",
    "\tdef __repr__(self):\n",
    "\t\treturn str({'location': self.location, 'coords': self.coords, 'length': self.length})\n",
    "\n",
    "def getBlackSquares(imgLoc,nSquares):\n",
    "\tim = Image.open(imgLoc)\n",
    "\tim = Image.composite(im, Image.new('RGB', im.size, 'white'), im)\n",
    "\tpps = im.size[0]/nSquares\n",
    "\treturn [(i,j) for i in range(nSquares) for j in range(nSquares) if not im.crop((j*pps,i*pps,j*pps+pps,i*pps+pps)).getbbox()]\n",
    "\n",
    "def getNewClueWithLength(i,j,n,blackSquares,direction,nSquares):\n",
    "\tif direction == \"Across\":\n",
    "\t\tnextBlack = [q for (p,q) in blackSquares if i == p]\n",
    "\t\tlen = (nextBlack[0] if nextBlack else nSquares) - j\n",
    "\telif direction == \"Down\":\n",
    "\t\tnextBlack = [p for (p,q) in blackSquares if j == q]\n",
    "\t\tlen = (nextBlack[0] if nextBlack else nSquares) - i\n",
    "\treturn Clue(direction,n,(i,j),len)\n",
    "\n",
    "def placeClues(blackSquares,nSquares):\n",
    "\tblack = \"  \"\n",
    "\tn = 1\n",
    "\tgrid = list()\n",
    "\tclues = {\n",
    "\t\t\"Across\": list(),\n",
    "\t\t\"Down\": list()\n",
    "\t}\n",
    "\tfor i in range(nSquares):\n",
    "\t\tgrid.append(list())\n",
    "\t\tfor j in range(nSquares):\n",
    "\t\t\tif len(blackSquares) > 0 and blackSquares[0] == (i,j):\n",
    "\t\t\t\tgrid[i].append(black)\n",
    "\t\t\t\tblackSquares.pop(0)\n",
    "\t\t\telse:\n",
    "\t\t\t\tif i == 0:\n",
    "\t\t\t\t\tif j == 0 or grid[i][j-1] == black:\n",
    "\t\t\t\t\t\tclues[\"Across\"].append(getNewClueWithLength(i,j,n,blackSquares,\"Across\",nSquares))\n",
    "\t\t\t\t\tgrid[i].append(zero(n))\n",
    "\t\t\t\t\tclues[\"Down\"].append(getNewClueWithLength(i,j,n,blackSquares,\"Down\",nSquares))\n",
    "\t\t\t\t\tn+=1\n",
    "\t\t\t\telif j == 0:\n",
    "\t\t\t\t\tif grid[i-1][j] == black:\n",
    "\t\t\t\t\t\tclues[\"Down\"].append(getNewClueWithLength(i,j,n,blackSquares,\"Down\",nSquares))\n",
    "\t\t\t\t\tgrid[i].append(zero(n))\n",
    "\t\t\t\t\tclues[\"Across\"].append(getNewClueWithLength(i,j,n,blackSquares,\"Across\",nSquares))\t\t\t\t\t\t\n",
    "\t\t\t\t\tn+=1\n",
    "\t\t\t\telif grid[i][j-1] == black:\n",
    "\t\t\t\t\tgrid[i].append(zero(n))\n",
    "\t\t\t\t\tclues[\"Across\"].append(getNewClueWithLength(i,j,n,blackSquares,\"Across\",nSquares))\n",
    "\t\t\t\t\tn+=1\n",
    "\t\t\t\telif grid[i-1][j] == black:\n",
    "\t\t\t\t\tgrid[i].append(zero(n))\n",
    "\t\t\t\t\tclues[\"Down\"].append(getNewClueWithLength(i,j,n,blackSquares,\"Down\",nSquares))\n",
    "\t\t\t\t\tn+=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tgrid[i].append(\"__\")\n",
    "\t\t# print(\" \".join(grid[i]))\n",
    "\treturn clues, grid\n",
    "\n",
    "def mergeWithClues():\n",
    "\tyear = \"2017\"\n",
    "\tiyear = int(year)\n",
    "\twith open('./data/data_'+year+'.json','r') as f:\n",
    "\t\tclues_from_text = f.read()\n",
    "\t\tclues_from_text = json.loads(clues_from_text)\n",
    "\tfor i in range(9,10):\n",
    "\t\tday = 25 #calendar.monthrange(int(year),i)[1]\n",
    "\t\tfor j in [x for x in range(day,day+1)]: ##  if x != 19 and x != 24 (unreadable files)\n",
    "\t\t\tnSquares = 21 if calendar.weekday(iyear, i, j) == 6 else 15\n",
    "\t\t\turl_date = zero(i)+zero(j)+\"-\"+year[2:]\n",
    "\t\t\tcal_date = year+\"-\"+zero(i)+'-'+zero(j)\n",
    "\t\t\tprint(url_date,cal_date)\n",
    "\t\t\tdate_clues_from_image, grid = placeClues(getBlackSquares('./images/'+url_date+'.png',nSquares),nSquares)\n",
    "\t\t\tdate_clues_from_text = [x for x in clues_from_text if x[\"date\"] == cal_date][0]['clues']\n",
    "\t\t\tmerge = list()\n",
    "\t\t\tfor dir in (\"Across\",\"Down\"):\n",
    "\t\t\t\tmerge += [{'location': a['location'], 'clue': a['clue'], 'answer': a['answer'], 'coords': b.coords, 'length': b.length} for a in date_clues_from_text[dir] for b in date_clues_from_image[dir] if a['location'] == b.location]\n",
    "\t\t\tdata = [{\"date\": cal_date, \"clues\": merge, \"grid\": grid}]\n",
    "\t\t\twith open('./data/merge_'+url_date+'.json','w') as f:\n",
    "\t\t\t\tf.write(json.dumps(data,indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0925-17 2017-09-25\n"
     ]
    }
   ],
   "source": [
    "mergeWithClues()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sortVocab ####\n",
    "* IN\n",
    "    * maxlen (int): longest word we want to store\n",
    "* OUT\n",
    "    * sortedvocab (dict): NLTK vocabulary list sorted by length\n",
    "    \n",
    "#### cosine ####\n",
    "* IN\n",
    "    * v1 (list): vector\n",
    "    * v2 (list): vector\n",
    "* OUT\n",
    "    * ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sortVocab(maxlen):\n",
    "    sortedvocab = {}\n",
    "    keys = []\n",
    "    for i in [w for w in nlp.vocab if w.has_vector and w.orth_.islower() and len(w.orth_) <= maxlen]:\n",
    "        k = len(i.orth_)\n",
    "        if k not in keys:\n",
    "            sortedvocab[k] = []\n",
    "            keys.append(k)\n",
    "        sortedvocab[k].append(i)\n",
    "    return sortedvocab\n",
    "\n",
    "cosine = lambda v1, v2: np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getKnowledgeGraphCandidates ####\n",
    "Uses Google Knowledge Graph Search API to return articles\n",
    "* IN\n",
    "    * clue (str): clue to pass as API query\n",
    "    * length (int): word length to filter response\n",
    "* OUT\n",
    "    * (set) words from API search of correct length\n",
    "\n",
    "#### getTokensForKGSearch\n",
    "* IN\n",
    "\t* clue (str): clue to get pos_\n",
    "\t* properonly (bool): include only proper nouns\n",
    "* OUT\n",
    "    * Tokenize clue return words with specific POS\n",
    "\n",
    "#### getWikiCandidates ####\n",
    "Uses Wikipedia API to return search results\n",
    "* IN\n",
    "    * clue (str): clue to pass as API search query\n",
    "    * length (int): word length to filter response\n",
    "* OUT\n",
    "    * (set) words from API search of correct length\n",
    "\n",
    "#### getWordnetCandidates ####\n",
    "* IN\n",
    "    * clue (str): one-word clue for which to find syno-,hypo-,hyper-nyms. \n",
    "    * length (int): length of expected answer\n",
    "* OUT\n",
    "    * Uses NLTK WordNet\n",
    "        \n",
    "#### getSpacyCandidates ####\n",
    "* IN\n",
    "    * clue (list): tokenized list of words whose vectors to sum\n",
    "    * length (int): length of expected answer\n",
    "    * ret_count (int): number of candidates to return\n",
    "* OUT\n",
    "    * Uses spaCy word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getKnowledgeGraphCandidates(clue,length):\n",
    "    api_key = 'AIzaSyCz3EetlDMLlyU7LLWUH2n1U7mhUfqyxRk'\n",
    "    service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
    "    params = {\n",
    "            'query': clue,\n",
    "            'limit': 10,\n",
    "            'indent': True,\n",
    "            'key': api_key,\n",
    "    }\n",
    "    url = service_url + '?' + urllib.parse.urlencode(params)\n",
    "    candidates = set()\n",
    "    response = json.loads(urllib.request.urlopen(url).read())\n",
    "    for element in response['itemListElement']:\n",
    "        if element[\"result\"].get(\"detailedDescription\",{}).get(\"articleBody\",None) is not None:\n",
    "            text = removePunct(element[\"result\"][\"detailedDescription\"][\"articleBody\"])\n",
    "            candidates.update(set(matchLength(text,length)))\n",
    "    return candidates\n",
    "\n",
    "def getTokensForKGSearch(clue,properonly = True):\n",
    "    clue = clue[:1].lower() + clue[1:]\n",
    "    tokens = nltk.pos_tag(nltk.word_tokenize(clue))\n",
    "    lookup = [\"NNP\",\"NNPS\"] if properonly else [\"NNP\",\"NNPS\",\"NN\",\"NNS\"]\n",
    "    ret = \" \".join([i for (i,j) in tokens if j in lookup])\n",
    "    return ret\n",
    "\n",
    "def getWikiCandidates(clue,length):\n",
    "    service_url = 'https://en.wikipedia.org/w/api.php?action=query&list=search&format=json'\n",
    "    search_string = urllib.parse.urlencode({\"srsearch\":clue})\n",
    "    url = service_url+'&'+search_string\n",
    "    response = json.loads(urllib.request.urlopen(url).read())\n",
    "    candidates = set()\n",
    "    for entry in response['query']['search']:\n",
    "        text = removePunct(entry['title'])\n",
    "        candidates.update(set(matchLength(text,length)))\n",
    "        text = removePunct(entry['snippet'])\n",
    "        candidates.update(set(matchLength(text,length)))\n",
    "    return candidates\n",
    "\n",
    "def getWordnetCandidates(clue,length):\n",
    "    candidates = set()\n",
    "    clue = re.sub('['+string.punctuation+']','',clue.lower())\n",
    "    if clue is not wn.morphy(clue):\n",
    "        morphedclue = wn.morphy(clue)\n",
    "        morph = clue.replace(morphedclue,'')\n",
    "        clue = morphedclue\n",
    "    synsets = wn.synsets(clue)\n",
    "    candidates.update({y for x in synsets for y in x.lemma_names() if len(y) == length})\n",
    "    if 'morph' in locals():\n",
    "        candidates.update({y+morph for x in synsets for y in x.lemma_names() if len(y) == length - len(morph)})\n",
    "    for syn in synsets:\n",
    "        if syn.hyponyms():\n",
    "            for hyposet in syn.hyponyms():\n",
    "                candidates.update({lemma.name() for lemma in hyposet.lemmas() if len(lemma.name()) == length})\n",
    "        if syn.hypernyms():\n",
    "            for hyperset in syn.hypernyms():\n",
    "                candidates.update({lemma.name() for lemma in hyperset.lemmas() if len(lemma.name()) == length})\n",
    "    return candidates\n",
    "\n",
    "def getSpacyCandidates(clue,length,vocab,ret_count):\n",
    "    vecs = [x.vector for x in clue]\n",
    "    vecsum = functools.reduce(lambda x,y: np.add(x,y),vecs)\n",
    "    vocab = [w for w in vocab if w not in clue]\n",
    "    vocab.sort(key=lambda w: cosine(w.vector, vecsum))\n",
    "    saveVectors(clue,vecsum,vocab[-25:])\n",
    "    return {w.orth_ for w in vocab[-1*ret_count:]}\n",
    "\n",
    "def saveVectors(clue,vecsum,vocab):\n",
    "    clue = \"_\".join([x.lower_ for x in clue])\n",
    "    data = list()\n",
    "    data.append({\"word\": clue, \"vector\": list(map(lambda x: x.item(),vecsum))})\n",
    "    for i in vocab:\n",
    "        data.append({\"word\": i.lower_, \"vector\": list(map(lambda x: x.item(),i.vector))})\n",
    "    with open('./vector_data/'+clue+'.json','w') as f:\n",
    "        f.write(json.dumps(data))\n",
    "\n",
    "def removePunct(text):\n",
    "    text = re.sub('é','e',text)\n",
    "    text = re.sub('à','a',text)\n",
    "    text = re.split('[^a-zA-Z]+',text)\n",
    "    return text\n",
    "\n",
    "def matchLength(text,length):\n",
    "    return filter(lambda x: len(x) == length, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### getCandidates ####\n",
    "Takes array of clues and appends candidate answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCandidates(clues):\n",
    "    for i,v in enumerate(clues):\n",
    "        clue = v['clue']\n",
    "        length = v['length']\n",
    "        print(clue,length)\n",
    "        if clue.find(' ') == -1:\n",
    "            v[\"cand_wn\"] = getWordnetCandidates(clue,length)\n",
    "            v[\"cand_wn\"] = list(v[\"cand_wn\"])\n",
    "#             print('Wordnet: ',v[\"cand_wn\"])\n",
    "        elif \"___\" in clue:\n",
    "            v[\"cand_wk\"] = set()\n",
    "            v[\"cand_wk\"].update(getWikiCandidates(re.sub(\"___\",\"\",clue),length))\n",
    "            v[\"cand_wk\"] = list(v[\"cand_wk\"])\n",
    "        elif re.search('([0-9]+)(\\-)(Across|Down)',clue) is not None:\n",
    "            print(\"x-A/y-D\",clue)\n",
    "        else:\n",
    "            formulations = []\n",
    "            formulations.append([nlp.vocab[x.lower_] for x in nlp(clue) if x.pos_ == \"NOUN\" or x.pos_ == \"PROPN\"])\n",
    "            formulations.append([nlp.vocab[x] for x in clue.split() if x not in stopwords.words('english')])\n",
    "            if len(formulations[0]) == 0 and len(formulations[1]) == 0:\n",
    "                formulations = list([nlp.vocab[x.lower_] for x in nlp(clue) if x.pos_ is not \"PART\"])\n",
    "            v[\"cand_vec\"] = set()\n",
    "            for clue_tokens in formulations:\n",
    "                if len(clue_tokens) != 0:\n",
    "                    v[\"cand_vec\"].update(getSpacyCandidates(clue_tokens,length,vocab[length],5))\n",
    "#             print('Word Vectors: ', v[\"cand_vec\"])\n",
    "\n",
    "            v[\"cand_kg\"] = set()\n",
    "            clue_tokens = getTokensForKGSearch(clue,True)\n",
    "            if clue_tokens != '':\n",
    "                v[\"cand_kg\"].update(getKnowledgeGraphCandidates(clue_tokens,length))\n",
    "            clue_tokens = getTokensForKGSearch(clue,False)\n",
    "            if clue_tokens != '':\n",
    "                v[\"cand_kg\"].update(getKnowledgeGraphCandidates(clue_tokens,length))\n",
    "#             print('Knowledge Graph: ', v[\"cand_kg\"])\n",
    "\n",
    "            v[\"cand_wk\"] = set()\n",
    "            v[\"cand_wk\"].update(getWikiCandidates(clue,length))\n",
    "#             print('Wikipedia Search: ', v[\"cand_kg\"])\n",
    "\n",
    "            v[\"cand_vec\"] = list(v[\"cand_vec\"])\n",
    "            v[\"cand_kg\"] = list(v[\"cand_kg\"])\n",
    "            v[\"cand_wk\"] = list(v[\"cand_wk\"])\n",
    "        clues[i] = v\n",
    "    return clues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loadPuzzle ####\n",
    "* IN\n",
    "\t* fileloc (str): path to JSON file\n",
    "* OUT\n",
    "    * writes file with clues+candidates appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadPuzzle(fileloc):\n",
    "    with open(fileloc,'r') as fr:\n",
    "        puzzle = json.loads(fr.read())[0]\n",
    "    puzzle['clues'] = getCandidates(puzzle['clues'])\n",
    "    if input(\"print to file? y/n: \") == 'y':\n",
    "        with open(fileloc[:-5]+\"_cands.json\",'w') as fw:\n",
    "            fw.write(json.dumps(puzzle,indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = sortVocab(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leatherworker's tool 3\n",
      "Wrath 3\n",
      "Sauce often used in a Bloody Mary 7\n",
      "Port-au-Prince resident 7\n",
      "\"Um-hmm, O.K.\" 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel/__main__.py:12: RuntimeWarning: invalid value encountered in float_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call from a football referee 7\n",
      "\"Please! Anything but!\" 7\n",
      "Onion relative used in soups 4\n",
      "Little troublemakers 4\n",
      "Charged particles 4\n",
      "M.R.I. orderers 3\n",
      "Versatile bean 4\n",
      "Texas site of a 1993 siege 4\n",
      "Itsy-bitsy branch 4\n",
      "Some DVD players 4\n",
      "Caustic agent 3\n",
      "Japan's largest company by revenue 6\n",
      "Crops used in making cigarettes 8\n",
      "Ready, willing and ___ 4\n",
      "Classic Eric Clapton song about unrequited love 5\n",
      "Statutes 4\n",
      "Loses one's hair 8\n",
      "Hold back, as a yawn 6\n",
      "Moment, informally 3\n",
      "World's fair, e.g 4\n",
      "Wish 4\n",
      "Like the water in a baptism 4\n",
      "Get bent out of shape 4\n",
      "___ talks (lecture series) 3\n",
      "Busy time at the drive-thru 4\n",
      "Nay voter 4\n",
      "It's made up of DNA 4\n",
      "Message that might end \"R.I.P.\" 7\n",
      "Invaded in large numbers 7\n",
      "17-year insects 7\n",
      "Standards by which things are measured 7\n",
      "Follows, as a schedule 7\n",
      "Monterrey Mrs 3\n",
      "Consumed 3\n",
      "Get ___ of (grasp) 5\n",
      "Communion tidbit 5\n",
      "Vegetarianism or bohemianism 15\n",
      "Three on a grandfather clock 3\n",
      "Source of faraway X-rays 11\n",
      "Foe 5\n",
      "Pantry containers 4\n",
      "Long, long ___ 3\n",
      "Web crawler, e.g 3\n",
      "Web-filled room, often 5\n",
      "Spot for a food fight 15\n",
      "Jackie of \"Shanghai Knights\" 4\n",
      "Honey Bunches of ___ 4\n",
      "\"Shameful!\" 3\n",
      "Really revel ... or a hint to the words formed by the circled letters 9\n",
      "White ___ sheet 3\n",
      "Troubles 4\n",
      "Kurtz's rank in \"Apocalypse Now\": Abbr 3\n",
      "Mournful cry 4\n",
      "To be, to Tacitus 4\n",
      "Little things that say \"To\" and \"From\" 4\n",
      "Orchestra reed 4\n",
      "Scissor cut 4\n",
      "Swiss mount 3\n",
      "___ Paese (variety of cheese) 3\n",
      "Spinning toy 3\n",
      "Like beer that's not in a bottle 5\n",
      "Things split in fission 5\n",
      "Make into 41-Across 5\n",
      "x-A/y-D Make into 41-Across\n",
      "Not the brightest bulb on the Christmas tree 5\n",
      "Part bitten by a vampire 4\n",
      "Mayberry boy 4\n",
      "Many online banners 3\n",
      "Bit of butter 3\n",
      "When a plane is due in, for short 3\n",
      "print to file? y/n: y\n"
     ]
    }
   ],
   "source": [
    "loadPuzzle('./data/merge_0102-17.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Match Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run some analytics on my candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cand_methods = ['cand_vec','cand_kg','cand_wn','cand_wk']\n",
    "\n",
    "def getHitCount(clues):\n",
    "    success = []\n",
    "    tally = []\n",
    "    for i in clues:\n",
    "        score = [0 for x in cand_methods]\n",
    "        add = False\n",
    "        for index,j in enumerate(cand_methods):\n",
    "            if i.get(j) and re.sub(\" \",\"\",i['answer'].lower()) in [x.lower() for x in i[j]]:\n",
    "#                 print(i['answer'],i[j],j)\n",
    "                add = True\n",
    "                score[index] = 1\n",
    "        if add:\n",
    "            success.append(i)\n",
    "            tally.append(score)\n",
    "    print(len(success),len(clues),len(success)/len(clues))\n",
    "    return success,tally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is my hit rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 74 0.36486486486486486\n"
     ]
    }
   ],
   "source": [
    "puzzle = json.load(open('./data/merge_0102-17_cands.json','r'))\n",
    "clues = puzzle['clues']\n",
    "success,tally = getHitCount(clues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which methods did the hits come from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         cand_vec  cand_kg  cand_wn  cand_wk\n",
      "AWL             1        0        0        1\n",
      "IRE             0        0        1        0\n",
      "TABASCO         0        1        0        1\n",
      "HAITIAN         1        0        0        1\n",
      "LEEK            0        0        0        1\n",
      "IONS            1        1        0        0\n",
      "SOYA            0        1        0        0\n",
      "WACO            0        0        0        1\n",
      "TWIG            1        0        0        0\n",
      "LYE             1        0        0        1\n",
      "ABLE            0        0        0        1\n",
      "LAYLA           0        0        0        1\n",
      "HOPE            0        0        1        0\n",
      "HOLY            1        0        0        1\n",
      "GENE            1        0        0        0\n",
      "CICADAS         1        1        0        1\n",
      "ENEMY           0        0        1        0\n",
      "AGO             0        0        0        1\n",
      "BOT             1        0        0        1\n",
      "CHAN            0        1        0        1\n",
      "OATS            0        0        0        1\n",
      "OBOE            1        0        0        0\n",
      "BEL             0        0        0        1\n",
      "TOP             0        0        0        1\n",
      "ATOMS           1        0        0        1\n",
      "OPIE            0        0        0        1\n",
      "ADS             1        0        0        1\n"
     ]
    }
   ],
   "source": [
    "tally = pd.DataFrame(tally,index=map(lambda x: x['answer'],success),columns=cand_methods)\n",
    "print(tally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do these hits actually look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\",200)\n",
    "df = pd.DataFrame(success)\n",
    "# print(df.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Reference the Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getCoordsLookup(puzzle):\n",
    "    clues = puzzle['clues']\n",
    "    clues = concatCands(clues)\n",
    "    grid = puzzle['grid']\n",
    "    lookup = dict()\n",
    "    for i,row in enumerate(grid):\n",
    "        for j,cell in enumerate(row):\n",
    "            if cell != \"  \":\n",
    "                entry = dict()\n",
    "                relevant_clues = [x for x in clues if [i,j] in x['coords']]\n",
    "                for clue in relevant_clues:\n",
    "                    position = clue['coords'].index([i,j])\n",
    "                    cands = [(x[position],x) for x in clue['cands']]\n",
    "                    for cand in cands:\n",
    "                        if not entry.get(cand[0]):\n",
    "                            entry[cand[0]] = {\"Across\":[],\"Down\":[]}\n",
    "                        entry[cand[0]][clue['location'].split(\"-\")[1]].append(cand[1])\n",
    "                lookup[str(i)+\"_\"+str(j)] = entry\n",
    "    return lookup\n",
    "\n",
    "def concatCands(clues):\n",
    "    for index,clue in enumerate(clues):\n",
    "        clue['cands'] = set()\n",
    "        for i in cand_methods:\n",
    "            if clue.get(i):\n",
    "                clue['cands'].update(clue[i])\n",
    "        clue['cands'] = [x.lower() for x in clue['cands']]\n",
    "        clues[index] = clue\n",
    "    return clues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getIntersections(puzzle):\n",
    "    clues = puzzle['clues']\n",
    "    clues = concatCands(clues)\n",
    "    grid = puzzle['grid']\n",
    "    intersections = dict()\n",
    "    for clue in clues:\n",
    "        direction = \"Down\" if clue['location'].split(\"-\")[1] == \"Across\" else \"Across\"\n",
    "        intersections[clue['location']] = dict()\n",
    "        for cand in clue['cands']:\n",
    "            intersections[clue['location']][cand] = 0\n",
    "            for i,coord in enumerate(clue['coords']):\n",
    "#                 print(cand,i,coord)\n",
    "                if len(coordslookup[str(coord[0])+\"_\"+str(coord[1])][cand[i]][direction]) > 0:\n",
    "#                     print(coordslookup[str(coord[0])+\"_\"+str(coord[1])][cand[i]][direction])\n",
    "                    intersections[clue['location']][cand] += 1\n",
    "            intersections[clue['location']][cand] = round(intersections[clue['location']][cand]/clue['length'],2)\n",
    "    return intersections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In each cell, what letters appear for candidates in both directions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1-Across': {'bot': 0.0, 'its': 0.33, 'can': 0.67, 'his': 0.67, 'the': 0.67, 'awl': 0.33, 'and': 0.33, 'ann': 0.33, 'age': 0.33, 'are': 0.33, 'all': 0.67, 'app': 0.33, 'cnc': 0.67, 'nwn': 0.33, 'jig': 0.33, 'for': 0.33, 'key': 0.33, 'now': 0.33, 'use': 0.67, 'web': 0.33, 'pin': 0.67}, '4-Across': {'ira': 0.33, 'ire': 0.67}, '7-Across': {'nothing': 0.71, 'chicken': 0.71, 'flavour': 0.29, 'survive': 0.57, 'peppers': 0.86, 'slasher': 0.57, 'legends': 0.86, 'fernand': 0.86, 'mixture': 0.71, 'pudding': 0.57, 'pacific': 0.43, 'liqueur': 0.43, 'cooking': 0.57, 'richard': 0.86, 'vampire': 0.86, 'instead': 0.86, 'sausage': 0.86, 'brought': 0.43, 'barbara': 0.71, 'tabasco': 1.0, 'overall': 0.57, 'popular': 0.29, 'because': 0.86, 'claimed': 0.71, 'younger': 0.43, 'vietnam': 0.86, 'usually': 0.71, 'imparts': 0.86, 'similar': 0.57, 'ketchup': 0.57, 'mustard': 0.71, 'musical': 0.71, 'written': 0.57, 'created': 0.71, 'ireland': 0.57, 'lifford': 0.86, 'planter': 0.43, 'cayenne': 0.71, 'england': 0.71, 'episode': 0.57, 'brother': 0.43, 'central': 1.0, 'rodgers': 0.86}, '14-Across': {'matteis': 0.57, 'evacuee': 0.43, 'capital': 0.29, 'haitian': 0.71, 'princes': 0.29, 'article': 0.43, 'divided': 0.29, 'widower': 0.43, 'settled': 0.43, 'angeles': 0.57, 'monarch': 0.43, 'reached': 0.57, 'domingo': 0.14, 'refugee': 0.43, 'toprens': 0.29, 'diverse': 0.29, 'citizen': 0.43, 'husband': 0.29, 'resided': 0.43, 'italian': 0.57, 'january': 0.57, 'country': 0.29, 'english': 0.43, 'brother': 0.71}, '16-Across': {'foiling': 0.57, 'kidding': 0.29, 'tyndall': 0.29, 'martino': 0.71, 'members': 0.57, 'digable': 0.71, 'western': 0.71, 'playing': 0.43, 'located': 0.71, 'michael': 0.43, 'academy': 0.43, 'hahahah': 0.86, 'mixtape': 0.57, 'soldati': 0.71, 'anyways': 0.43, 'freshly': 0.43, 'bulkley': 0.57, 'outlaws': 0.71, 'reached': 0.71, 'planets': 0.71, 'tyndale': 0.43, 'chicken': 0.43, 'spelled': 0.57, 'chandon': 0.57, 'villain': 0.57, 'connery': 0.71, 'october': 0.71, 'receive': 0.57, 'obliged': 0.57, 'arizona': 0.43, 'foreign': 0.57, 'hmmmmmm': 0.29, 'between': 0.71, 'pro-big': 0.43, 'italian': 0.71, 'british': 0.43, 'terrace': 0.57, 'douglas': 0.29, 'loosely': 0.71, 'unicare': 0.71, 'fingers': 0.71, 'alright': 0.57, 'germany': 0.57, 'alberto': 0.57, 'ladybug': 0.43, 'brother': 0.71, 'cowboys': 0.43}, '17-Across': {'coaches': 0.14, 'calling': 0.43, 'present': 0.43, 'thomson': 0.29, 'dispute': 0.71, 'kayfabe': 0.43, 'stephen': 0.29, 'captain': 0.43, 'players': 0.29, 'purpose': 0.29, 'stadium': 0.43, 'regular': 0.14, 'retired': 0.29, 'similar': 0.57, 'paisley': 0.14, 'between': 0.29, 'matches': 0.43, 'referee': 0.29, 'english': 0.29, 'amateur': 0.43, 'leagues': 0.14, 'lockout': 0.43, 'umpires': 0.29}, '18-Across': {'nothing': 0.57, 'anybody': 0.71, 'someone': 0.71, 'anymore': 0.71, 'because': 0.71}, '19-Across': {'stew': 0.75, 'meat': 0.5, 'tofu': 0.25, 'that': 0.25, 'much': 0.5, 'also': 0.25, 'late': 0.5, 'from': 0.0, 'span': 0.5, 'leek': 0.5, 'show': 0.25, 'beef': 0.5, 'dill': 0.5, 'foot': 0.25, 'used': 0.75, 'quot': 0.25, 'root': 0.5, 'were': 0.25, 'calf': 0.25, 'sour': 0.5, 'fish': 0.25, 'well': 0.25, 'soup': 0.5, 'clam': 0.5, 'meal': 0.5, 'beet': 0.5, 'peas': 0.25, 'only': 0.25, 'stir': 0.5, 'list': 0.5, 'rice': 0.5, 'burt': 0.5, 'with': 0.5, 'west': 0.25, 'they': 0.25, 'like': 0.5, 'very': 0.0, 'main': 0.5}, '20-Across': {'onto': 0.5, 'fred': 0.25, 'that': 0.5, 'your': 0.0, 'imdb': 0.5, 'wood': 0.5, 'love': 0.25, 'song': 0.0, 'yobs': 0.25, 'when': 0.25, 'socs': 0.5, 'step': 0.25, 'some': 0.25, 'girl': 0.5, 'from': 0.5, 'span': 0.5, 'late': 0.5, 'more': 0.5, 'scum': 0.25, 'olly': 0.75, 'ones': 0.5, 'film': 0.5, 'seem': 0.25, 'tiny': 0.25, 'band': 0.5, 'quot': 0.5, 'punk': 0.25, 'land': 0.5, 'york': 0.25, 'make': 0.5, 'hill': 0.5, 'bums': 0.25, 'part': 1.0, 'live': 0.25, 'they': 0.25, 'game': 0.5, 'taio': 0.75, 'oafs': 0.75, 'murs': 0.5, 'lead': 0.5, 'card': 0.75, 'what': 0.5, 'last': 0.75, 'with': 0.25, 'cruz': 0.0, 'call': 0.75, 'pony': 0.5, 'them': 0.25, 'time': 0.25}, '22-Across': {'dust': 0.75, 'acid': 0.5, 'uric': 0.75, 'july': 0.5, 'that': 0.75, 'flux': 0.25, 'much': 0.75, 'rest': 1.0, 'also': 0.5, 'jack': 0.75, 'than': 0.75, 'when': 0.75, 'beam': 0.75, 'from': 1.0, 'span': 0.75, 'many': 1.0, 'such': 0.75, 'soot': 1.0, 'hank': 1.0, 'blue': 0.25, 'film': 1.0, 'tiny': 1.0, 'near': 1.0, 'life': 1.0, 'moon': 1.0, 'five': 0.75, 'type': 0.75, 'make': 1.0, 'atom': 0.75, 'most': 1.0, 'hard': 0.75, 'hill': 1.0, 'high': 1.0, 'ions': 1.0, 'hart': 1.0, 'gray': 1.0, 'they': 0.75, 'labs': 0.75, 'stan': 0.75, 'thus': 0.5, 'bell': 0.75, 'only': 0.75, 'list': 1.0, 'hplc': 0.5, 'with': 1.0, 'mass': 1.0, 'very': 1.0, 'gale': 1.0}, '23-Across': {'yks': 0.33, 'its': 0.33, 'hat': 0.33, 'rev': 0.67, 'are': 0.67, 'f9r': 0.0, 'yoj': 0.33, 'was': 0.0, 'the': 0.67, 'sum': 0.33, 'for': 0.0, 'who': 0.67, '8er': 0.33, 'win': 0.33, 'new': 0.67, 'ell': 0.67, 'oay': 0.33, 'and': 0.67, 'has': 0.0}, '24-Across': {'acid': 0.25, 'food': 0.25, 'role': 0.75, 'milk': 0.25, 'home': 0.5, 'diet': 0.25, 'urad': 0.25, 'also': 0.5, 'gear': 0.5, 'tofu': 0.5, 'spot': 0.25, 'anti': 0.0, 'curd': 0.0, 'from': 0.25, 'dish': 0.0, 'many': 0.25, 'span': 0.25, 'next': 0.5, 'amos': 0.0, 'then': 0.25, 'more': 0.5, 'gets': 0.25, 'used': 0.25, 'film': 0.25, 'quot': 0.25, 'band': 0.25, 'lime': 0.25, 'faba': 0.5, 'bean': 0.5, 'seed': 0.25, 'bell': 0.5, 'name': 0.5, 'long': 0.5, 'east': 0.5, 'uses': 0.25, 'snap': 0.0, 'were': 0.5, 'pods': 0.25, 'part': 0.5, 'fish': 0.0, 'soup': 0.25, 'have': 0.5, 'bark': 0.25, 'peas': 0.25, 'mung': 0.25, 'made': 0.5, 'corn': 0.5, 'gram': 0.25, 'both': 0.25, 'fava': 0.5, 'rice': 0.25, 'list': 0.25, 'with': 0.0, 'they': 0.0, 'like': 0.25, 'very': 0.25, 'soya': 0.75, 'asia': 0.5}, '25-Across': {'john': 0.25, 'city': 0.5, 'that': 0.25, 'home': 0.5, 'waco': 0.5, 'here': 0.5, 'line': 0.75, 'also': 0.5, 'news': 0.0, 'huff': 0.25, 'from': 0.25, 'span': 0.5, 'best': 0.25, 'unit': 0.25, 'what': 0.25, 'some': 0.5, 'took': 0.0, 'film': 0.75, 'quot': 0.0, 'near': 0.5, 'army': 0.25, 'fort': 0.25, 'brig': 0.0, 'soft': 0.25, 'land': 0.5, 'name': 0.75, 'site': 0.75, 'four': 0.25, 'uses': 0.5, 'were': 0.5, 'ohio': 0.0, 'part': 0.5, 'ford': 0.25, 'sect': 0.25, 'fork': 0.25, 'list': 0.5, 'utah': 0.5, 'east': 0.5, 'with': 0.5, 'wake': 0.75, 'over': 0.5}, '27-Across': {'born': 0.0, 'doug': 0.25, 'cars': 0.25, 'that': 0.25, 'city': 0.0, 'post': 0.25, 'waco': 0.25, 'rain': 0.25, 'song': 0.25, 'hugo': 0.0, 'into': 0.25, 'came': 0.5, 'work': 0.0, 'from': 0.0, 'club': 0.0, 'span': 0.25, 'anna': 0.25, 'area': 0.25, 'kaye': 0.5, 'film': 0.0, 'tiny': 0.0, 'quot': 0.0, 'poet': 0.25, 'sctv': 0.5, 'eeny': 0.25, 'twig': 0.25, 'long': 0.5, 'four': 0.25, 'most': 0.0, \"li'l\": 0.25, 'navy': 0.25, 'itsy': 0.25, 'eric': 0.5, 'part': 0.5, 'best': 0.0, 'bank': 0.25, 'itty': 0.25, 'text': 0.0, 'both': 0.25, 'book': 0.0, 'list': 0.25, 'tree': 0.25, 'east': 0.5, 'with': 0.25, 'they': 0.25, 'down': 0.0, 'main': 0.25, 'team': 0.0}, '29-Across': {'city': 0.25, 'that': 0.75, 'home': 0.75, 'tale': 0.75, 'sony': 0.5, 'song': 0.5, 'also': 0.5, 'mack': 0.5, 'than': 0.75, 'into': 0.5, 'came': 0.5, 'pink': 0.25, 'mpeg': 0.25, 'some': 0.75, 'dual': 0.0, 'from': 0.5, 'tape': 0.75, 'many': 0.5, 'each': 0.75, 'span': 0.5, 'sell': 0.25, 'this': 0.5, 'such': 0.5, 'code': 0.5, 'game': 0.5, 'same': 0.75, 'film': 0.5, 'band': 0.75, 'raze': 0.75, 'hell': 0.25, 'will': 0.25, 'ipod': 0.25, 'even': 0.5, 'dvds': 0.0, 'tour': 0.5, 'name': 0.75, 'mice': 0.75, 'most': 0.75, 'were': 0.25, 'self': 0.25, 'head': 0.5, 'high': 0.75, 'play': 0.0, 'free': 0.75, 'well': 0.0, 'have': 0.75, 'full': 0.25, 'bert': 0.5, 'bell': 0.25, 'walt': 0.5, 'list': 0.5, 'long': 0.25, 'with': 0.5, 'disc': 0.25, 'been': 0.5, 'mass': 0.5, 'four': 0.5}, '31-Across': {'dye': 0.33, 'far': 0.67, 'set': 1.0, 'one': 0.33, 'the': 0.33, 'and': 0.33, 'scn': 1.0, 'any': 0.0, 'war': 0.67, 'are': 0.33, 'was': 0.67, 'oil': 1.0, 'act': 0.67, 'wao': 0.33, 'lye': 0.33, 'for': 0.67, 'nor': 1.0, 'law': 0.67, 'sis': 1.0, 'its': 0.67, 'two': 0.33, 'wet': 0.67, 'dry': 0.33, 'amp': 0.67, 'lee': 0.33, 'air': 0.67, 'may': 0.67, 'has': 0.67, 'off': 1.0, 'use': 0.33, 'but': 0.67, 'not': 1.0, 'sgt': 1.0, 'abc': 0.67, 'who': 0.33, 'spy': 0.67, 'red': 0.67, 'hot': 0.67}, '34-Across': {'profit': 0.33, 'sizing': 0.5, 'annual': 0.5, 'demand': 0.5, 'market': 0.5, 'canada': 0.67, 'either': 0.67, 'copper': 0.5, 'change': 0.33, 'volume': 0.33, 'people': 0.33, 'assets': 0.67, 'public': 0.33, 'single': 0.33, 'global': 0.33, 'income': 0.33, 'though': 0.83, 'futagi': 0.83, 'paying': 0.33}, '36-Across': {'cannabis': 0.75, 'exploded': 0.25, 'possibly': 0.62, 'soybeans': 0.75, 'although': 0.12, 'prepared': 0.38, 'educated': 0.38, 'marshall': 0.75, 'imported': 0.25, 'utilized': 0.38, 'industry': 0.62, 'modified': 0.38, 'anything': 0.5, 'consumed': 0.62, 'actually': 0.38, 'nicotine': 0.5, 'flavored': 0.25, 'products': 0.5}, '38-Across': {'tony': 0.5, 'dick': 0.5, 'rock': 0.75, 'were': 0.25, 'hard': 0.5, 'self': 0.5, 'jess': 0.5, 'song': 0.25, 'film': 0.5, 'quot': 0.25, 'band': 0.25, 'ruby': 0.5, 'ross': 0.75, 'abel': 0.75, 'jody': 0.25, 'wild': 0.5, 'lari': 0.5, 'able': 0.75, 'alan': 0.75, 'span': 0.5}, '39-Across': {'first': 0.6, 'about': 0.4, 'group': 0.4, 'heart': 0.6, 'bands': 0.6, 'three': 0.8, 'keith': 0.4, 'march': 0.4, 'angst': 0.6, 'dirty': 0.6, 'woman': 0.2, 'music': 0.6, 'bobby': 0.2, 'lyric': 0.4, 'junor': 0.6, 'plays': 0.4, 'cruel': 0.4, 'april': 0.8, 'world': 0.4, 'today': 0.4, 'their': 0.8, 'under': 0.4, 'after': 0.8, 'patti': 0.6, 'songs': 0.4, 'front': 0.4, 'blues': 0.4, 'bruce': 0.0, 'model': 0.4, 'abuse': 0.4, 'other': 0.6, 'loved': 0.4, 'penny': 0.6, 'third': 0.6, 'these': 0.8, 'power': 0.4, 'layla': 0.2, 'class': 0.8, 'track': 0.4, 'mitch': 0.4, 'dylan': 0.0, 'debut': 0.2, 'sorry': 0.6, 'album': 0.4, 'bleak': 0.4, 'baker': 0.4, 'cream': 0.4, 'derek': 0.6}, '41-Across': {'fisa': 0.5}, '42-Across': {'aquarius': 0.5, 'produced': 0.25, 'ponytail': 0.38, 'american': 0.5, 'reviewed': 0.38, 'physical': 0.25, 'erlewine': 0.75, 'narrated': 0.75, 'whatever': 0.38, 'eyebrows': 0.38, 'includes': 0.5, 'interest': 0.75, 'axillary': 0.5, 'received': 0.62, 'sunshine': 0.75, 'expanded': 0.25, 'comedian': 0.75, 'directed': 0.62, 'forehead': 0.88, 'straight': 0.62, 'virginia': 0.62, 'suddenly': 0.5, 'touching': 0.75, 'starring': 0.5, 'haircuts': 0.62, 'actually': 0.75, 'broadway': 0.25, 'surgical': 0.5, 'saturday': 0.5, 'terminal': 0.62, 'routines': 0.75, 'classics': 0.75, 'allmusic': 0.38, 'anything': 0.62}, '44-Across': {'eating': 0.67, 'lifted': 0.67, 'sister': 0.5, 'people': 0.5, 'meters': 0.67, 'steady': 0.67, 'sneeze': 0.5, 'rather': 0.5, 'seeing': 0.5, 'yawned': 0.67, 'review': 0.5, 'tongue': 0.67, 'fishin': 0.5, 'crater': 0.67, 'should': 0.67, 'giggle': 0.5, 'thinks': 0.67, 'bigpaw': 0.17, 'moving': 0.67, 'shadow': 0.67, 'asleep': 0.67, 'fallen': 0.67, 'guffaw': 0.33, 'second': 0.5, 'humans': 0.5, 'titled': 0.5, 'scream': 0.67, 'breath': 0.67, 'enough': 0.5, 'mumble': 0.5}, '46-Across': {'irc': 0.33, 'end': 0.67, 'for': 0.33, 'his': 0.33, 'rfc': 0.33, 'mtv': 0.67, 'que': 0.0, 'one': 0.33, 'mot': 1.0, 'the': 0.67, 'tcp': 0.33, 'and': 0.33, 'you': 1.0, 'let': 1.0, 'pop': 0.33, 'all': 0.67, 'gut': 0.67, 'was': 0.67, 'rio': 0.33, 'sad': 0.67, 'roy': 0.67, 'way': 0.67, 'sul': 0.67, 'top': 0.67, 'its': 0.33, 'jaw': 0.33, 'two': 0.33, 'had': 0.67, 'met': 1.0, 'mio': 0.33, 'los': 0.67, 'wtf': 0.67, 'has': 0.67, 'war': 0.67, 'zac': 0.33, 'but': 0.33, 'isp': 0.33, 'who': 0.67, 'tom': 0.67, 'see': 0.67}, '47-Across': {'city': 0.25, 'home': 0.75, 'also': 0.5, 'fair': 0.0, 'into': 0.75, 'some': 0.5, 'etc.': 0.5, 'from': 0.75, 'span': 0.5, 'many': 0.0, 'f.e.': 0.25, 'then': 0.75, 'show': 0.5, 'i.e.': 0.5, 'mind': 0.0, 'ever': 0.75, 'what': 0.75, 'such': 0.25, 'game': 0.25, 'life': 0.25, 'baku': 0.0, 'york': 0.5, 'site': 0.5, 'most': 0.75, 'were': 0.75, 'have': 0.5, 'park': 0.5, 'f.ex': 0.25, 'e.g.': 0.5, 'firm': 0.5, 'gate': 0.5, 'book': 0.5, 'only': 0.5, 'eggs': 0.75, 'open': 0.5, 'been': 0.75, 'http': 0.5, 'less': 0.75}, '48-Across': {'hope': 0.5, 'care': 0.25, 'like': 0.5, 'wish': 0.5, 'want': 0.5}, '49-Across': {'idea': 0.5, 'john': 0.25, 'that': 0.75, 'rain': 0.5, 'duly': 0.5, 'fact': 0.5, 'also': 0.75, 'holy': 0.5, 'when': 0.5, 'than': 0.75, 'from': 0.25, 'many': 0.5, 'wash': 0.75, 'more': 0.25, 'span': 0.75, 'fire': 0.5, 'this': 0.5, 'body': 0.5, 'used': 0.5, 'pool': 0.25, 'quot': 0.5, 'word': 0.0, 'even': 0.75, 'rite': 0.75, 'uses': 0.75, 'head': 0.25, 'they': 0.75, 'deep': 0.25, 'pond': 0.0, 'with': 0.75, 'like': 0.75, 'font': 0.25, 'salt': 0.75, 'over': 0.5}, '51-Across': {'pipe': 0.5, 'john': 0.25, 'oval': 0.5, 'year': 0.5, 'opus': 0.75, 'that': 0.5, 'home': 0.25, 'here': 0.5, 'xbox': 0.25, 'robo': 0.25, 'turn': 0.5, 'song': 0.5, 'also': 0.25, 'size': 0.75, 'into': 0.5, 'poem': 0.0, 'anti': 0.25, 'held': 0.5, 'some': 0.5, 'from': 0.25, 'span': 0.75, 'moon': 0.25, 'pull': 0.25, 'many': 0.5, 'more': 0.25, 'then': 0.5, 'expo': 0.5, 'with': 0.25, 'days': 0.5, 'best': 0.5, 'body': 0.25, 'game': 0.5, 'told': 0.5, 'tube': 0.5, 'ovid': 0.5, 'used': 0.5, 'quot': 0.25, 'shin': 0.5, 'bent': 0.5, 'folk': 0.0, 'life': 0.75, 'solo': 0.5, 'knot': 0.25, 'band': 0.5, 'type': 0.5, 'long': 0.5, 'name': 0.75, 'four': 0.25, 'most': 0.25, 'were': 0.5, 'hard': 0.5, 'ways': 0.5, 'head': 0.5, 'slot': 0.5, 'have': 0.5, 'poet': 0.25, 'form': 0.0, 'thus': 0.5, 'just': 0.25, 'axis': 0.25, 'made': 0.5, 'rock': 0.0, 'only': 0.5, 'gees': 0.5, 'ages': 0.25, 'this': 0.5, 'wood': 0.25, 'bend': 0.5, 'they': 0.5, 'edge': 0.75, 'look': 0.25, 'want': 0.5, 'over': 0.5}, '53-Across': {'day': 0.0, 'mid': 0.0, 'are': 0.33, 'not': 0.67, 'the': 0.67, 'for': 0.33, 'can': 0.0, 'amp': 0.33, 'bbc': 0.33, 'neh': 0.67, 'six': 0.0, 'art': 0.33, 'and': 0.0, 'com': 0.33, 'has': 0.33}, '56-Across': {'view': 0.25, 'imdb': 0.5, 'year': 0.5, 'cars': 0.5, 'line': 0.75, 'data': 1.0, 'thru': 0.5, 'when': 0.5, 'west': 0.25, 'busy': 0.5, 'size': 0.5, 'some': 0.75, 'hong': 0.75, 'span': 0.5, 'from': 0.25, 'road': 0.75, 'such': 0.5, 'plan': 0.5, 'quot': 0.0, 'hell': 0.75, 'ride': 0.5, 'deal': 0.5, 'four': 1.0, 'most': 0.5, 'hard': 0.75, 'left': 0.25, 'were': 0.5, 'well': 0.75, 'full': 0.75, 'once': 0.5, 'back': 0.5, 'just': 0.0, 'only': 0.75, 'stop': 0.75, 'wait': 0.5, 'this': 0.5, 'path': 1.0, 'list': 0.25, 'with': 0.75, 'true': 0.75, 'time': 0.5, 'huey': 0.5, 'they': 0.75, 'kong': 0.75}, '58-Across': {'ohio': 0.25, 'bill': 0.25, 'dems': 0.25, 'tony': 0.0, 'that': 0.25, 'poll': 0.25, 'veto': 0.5, 'call': 0.25, 'also': 0.0, 'myth': 0.5, 'than': 0.25, 'when': 0.25, 'span': 0.25, 'dare': 0.0, 'more': 0.0, 'each': 0.0, 'nays': 0.0, 'down': 0.0, 'roll': 0.25, 'quot': 0.25, 'lest': 0.25, 'vote': 0.25, 'laws': 0.0, 'make': 0.0, 'name': 0.0, 'live': 0.0, 'june': 0.0, 'were': 0.25, 'head': 0.5, 'ronr': 0.0, 'bias': 0.25, 'form': 0.0, 'meet': 0.5, 'just': 0.0, 'made': 0.0, 'book': 0.25, 'yeas': 0.5, 'chen': 0.25, 'egon': 0.5}, '59-Across': {'acid': 0.0, 'adsl': 0.25, 'girl': 0.5, 'htat': 0.25, 'that': 0.25, 'hola': 0.25, 'also': 0.25, 'tank': 0.25, 'than': 0.25, 'work': 0.0, 'thta': 0.25, 'some': 0.25, 'code': 0.25, 'from': 0.0, 'mrna': 0.25, \"it's\": 0.0, 'many': 0.25, 'gene': 0.25, 'labs': 0.25, 'span': 0.25, 'rgen': 0.25, 'coud': 0.0, 'this': 0.0, 'such': 0.25, 'arto': 0.25, 'unit': 0.25, 'thst': 0.25, 'used': 0.0, 'took': 0.0, 'film': 0.25, 'band': 0.25, 'holb': 0.0, 'each': 0.5, 'life': 0.5, 'mark': 0.25, 'five': 0.5, 'jump': 0.25, 'long': 0.0, 'uses': 0.0, 'cell': 0.25, 'they': 0.0, 'loci': 0.25, 'term': 0.0, 'made': 0.5, 'only': 0.0, 'mesa': 0.25, 'syco': 0.25, 'wave': 0.5}, '60-Across': {'against': 0.57, 'chapter': 0.43, 'nothing': 0.43, 'attempt': 0.43, 'buckley': 0.29, 'showbox': 0.14, 'wolfman': 0.29, 'michael': 0.43, 'pointed': 0.71, 'coroner': 0.71, 'seattle': 0.43, 'thought': 0.14, 'reunion': 0.43, 'perhaps': 0.71, 'typical': 0.57, 'messege': 0.29, 'because': 0.43, 'october': 0.43, 'actress': 0.57, 'vagrant': 0.71, 'destroy': 0.29, 'written': 0.71, 'several': 0.43, 'aligned': 0.86, 'january': 0.29, 'british': 0.57, 'rubicon': 0.57, 'peretti': 0.86, 'jacques': 0.29, 'jackson': 0.29, 'aaliyah': 0.57, 'message': 0.29, 'romance': 0.71, 'micheal': 0.43, 'goodbye': 0.29, 'william': 0.43, 'ruppert': 0.43, 'records': 0.43, 'angular': 0.43}, '63-Across': {'atlanta': 0.29, 'nicolas': 0.29, 'special': 0.43, 'version': 0.14, 'cabinet': 0.43, 'related': 0.29, 'counted': 0.43, 'present': 0.29, 'follows': 0.29, 'charlie': 0.57, 'located': 0.29, 'settled': 0.57, 'detroit': 0.57, 'numbers': 0.29, 'vessels': 0.29, 'history': 0.43, 'station': 0.43, 'invader': 0.43, 'numeric': 0.14, 'drummer': 0.43, 'indians': 0.57, 'shelter': 0.43, 'smaller': 0.57, 'prodigy': 0.29, 'because': 0.14, 'capture': 0.29, 'display': 0.57, 'changed': 0.43, 'falacci': 0.14, 'invaded': 0.43, 'usually': 0.57, 'vietnam': 0.29, 'several': 0.43, 'figures': 0.14, 'persian': 0.14, 'compete': 0.43, 'classic': 0.29, 'written': 0.43, 'january': 0.43, 'records': 0.29, 'created': 0.43, 'college': 0.29, 'british': 0.29, 'artwork': 0.43, 'looking': 0.43, 'complex': 0.43, 'quickly': 0.29, 'average': 0.57, 'country': 0.43, 'english': 0.29, 'working': 0.29, 'balance': 0.14, 'massive': 0.29, 'network': 0.29, 'episode': 0.57, 'daltrey': 0.29, 'islands': 0.29, 'brother': 0.29, 'central': 0.43}, '65-Across': {'million': 0.57, '13-year': 0.29, 'insects': 0.57, 'carmine': 0.71, 'natural': 0.57, 'whether': 0.14, 'control': 0.57, 'related': 0.43, 'insecta': 0.57, 'members': 0.29, 'rodents': 0.57, 'locusts': 0.43, 'whereby': 0.29, 'jointed': 0.43, 'reduced': 0.14, 'applied': 0.43, 'certain': 0.57, 'closely': 0.57, 'sterile': 0.43, 'opuntia': 0.71, 'america': 0.57, 'hepburn': 0.0, 'cicadas': 0.71, 'phagein': 0.43, 'numbers': 0.29, 'species': 0.29, 'walking': 0.43, 'arizona': 0.71, 'hexapod': 0.29, 'diptera': 0.43, 'evolved': 0.43, 'derived': 0.43, 'written': 0.29, 'culture': 0.14, 'beetles': 0.14, 'lizards': 0.57, 'spiders': 0.43, 'mammals': 0.57, 'entomon': 0.43, 'aspects': 0.43, 'protura': 0.14, 'diplura': 0.43, 'feeding': 0.14, 'sessile': 0.43}, '66-Across': {'nothing': 0.43, 'results': 0.43, 'achieve': 0.57, 'renwick': 0.43, 'encoded': 0.43, 'pioneer': 0.43, 'matters': 0.57, 'certain': 0.43, 'sensors': 0.43, 'because': 0.29, 'reasons': 0.43, 'decades': 0.29, 'largely': 0.43, 'factors': 0.43, 'century': 0.29, 'written': 0.29, 'british': 0.29, 'mistake': 0.29, 'matches': 0.43, 'speaker': 0.43, 'service': 0.43, 'reflect': 0.57, 'setting': 0.43, 'measure': 0.43, 'product': 0.29, 'exactly': 0.29, 'history': 0.0, 'density': 0.14}, '67-Across': {'without': 0.14, 'follows': 0.29, 'tickets': 0.57, 'service': 0.29, 'reduced': 0.57, 'further': 0.43, 'weekend': 0.57, 'brought': 0.0, 'players': 0.43, 'cartoon': 0.29, 'rosters': 0.57, 'regular': 0.43, 'variety': 0.29, 'counter': 0.29, 'clinton': 0.43, 'planned': 0.43, 'working': 0.14, 'company': 0.14, 'network': 0.43, 'morning': 0.14, 'through': 0.57}, '68-Across': {'for': 0.67, 'del': 0.67, 'rez': 0.33, 'one': 0.33, 'ann': 0.33, 'bbc': 0.0, 'his': 0.67, 'the': 0.67, 'and': 0.33, 'are': 0.33, 'mr.': 0.0, 'was': 0.33, 'mrs': 0.33, 'she': 0.67, 'new': 0.67, 'amy': 0.33, 'her': 0.67, 'its': 0.67, 'amp': 0.33, 'lee': 0.67, 'san': 0.67, 'rey': 0.67, 'mty': 0.33, 'rei': 0.33, 'who': 0.33, 'unt': 0.33, 'see': 0.67, 'frs': 0.67}, '69-Across': {'hit': 0.33, 'use': 0.33, 'eat': 0.33, 'try': 0.33, 'sap': 0.33, 'sup': 0.33}, '1-Down': {'grasp': 0.6, 'about': 0.8, 'chart': 1.0, 'hymie': 0.6, 'drama': 0.8, 'wrist': 0.8, 'tight': 0.6, 'paint': 0.6, 'offer': 0.2, 'lycra': 0.6, 'their': 0.8, 'audio': 0.6, 'least': 0.8, 'grabs': 0.8, 'until': 0.4, 'other': 0.4, 'krazy': 0.8, 'tells': 1.0, 'study': 0.8, 'field': 0.8, 'every': 0.6, 'shows': 0.6, 'sizes': 0.4, 'class': 0.8, 'nylon': 0.8, 'mouse': 0.6, 'coins': 0.4, 'easel': 0.4, 'smart': 0.8}, '2-Down': {'union': 0.8, 'class': 0.4, 'tagle': 0.8, 'laity': 0.8, 'faith': 0.8, 'first': 0.8, 'feast': 0.8, 'tasty': 0.6, 'basis': 0.4, 'altar': 0.8, 'irish': 1.0, 'hague': 0.8, 'sanas': 0.8, 'unity': 0.6, 'smash': 0.8, 'vince': 0.6}, '3-Down': {'intellectualism': 0.87, 'countercultural': 0.87, 'cosmopolitanism': 0.93, 'humanitarianism': 0.87, 'counter-culture': 0.8}, '4-Down': {'dad': 0.33, 'for': 0.33, 'old': 0.33, 'one': 0.67, 'his': 0.67, 'son': 0.0, 'the': 0.67, 'and': 0.33, 'are': 0.67, 'was': 0.67, 'six': 0.33, 'mom': 0.0, 'two': 0.67, 'cup': 0.33, 'had': 0.33, 'ten': 0.33, 'job': 0.0, 'red': 0.33, 'see': 0.67, 'via': 0.67}, '5-Down': {'ultraviolet': 0.82, 'wavelengths': 0.64, 'temperature': 0.82, 'searchmatch': 0.64, 'radioactive': 0.91, 'nanoscience': 0.73, 'fluorescent': 0.91, 'irradiation': 0.64, 'microphones': 0.64, 'radiography': 0.82, 'radiographs': 0.82}, '6-Down': {'rival': 0.4, 'enemy': 1.0}, '7-Down': {'food': 0.75, 'wide': 0.75, 'much': 0.75, 'also': 0.75, 'need': 0.5, 'half': 0.25, 'corn': 0.75, 'jars': 0.5, 'from': 0.75, 'dish': 0.75, 'many': 1.0, 'span': 1.0, 'sell': 0.5, 'code': 0.75, 'side': 0.75, 'used': 0.75, 'cans': 1.0, 'film': 0.75, 'quot': 0.5, 'ants': 0.25, 'bins': 1.0, 'name': 0.75, 'hand': 0.75, 'most': 1.0, 'were': 0.5, 'have': 0.5, 'full': 0.75, 'team': 0.75, 'they': 0.75, 'rfid': 0.75, 'lids': 0.75, 'made': 0.75, 'help': 0.25, 'mill': 0.75, 'with': 0.75, 'such': 0.75, 'like': 0.75, 'able': 0.5, 'moth': 0.75, 'very': 0.5, 'tray': 1.0, 'less': 0.75}, '8-Down': {'hit': 0.67, 'hot': 0.67, 'war': 0.33, 'its': 0.67, 'see': 1.0, 'end': 0.67, 'for': 0.33, 'new': 0.67, 'was': 0.33, 'the': 0.67, 'way': 0.33, 'ago': 0.67, 'buy': 0.33, 'man': 1.0, 'who': 0.67, 'now': 0.67, 'not': 0.67, 'and': 0.67}, '9-Down': {'bot': 1.0, 'for': 0.67, 'seo': 0.67, 'php': 0.67, 'i.e': 0.33, 'web': 0.33, 'the': 0.67, 'and': 0.67, 'are': 0.67, 'all': 0.67, 'gas': 0.67, 'per': 0.67, 'cgi': 0.67, 'ili': 0.67, 'non': 0.67, 'use': 0.67, 'rss': 0.67, 'etc': 1.0, 'dns': 0.67, 'see': 0.67}, '10-Down': {'ceuta': 0.8, 'about': 0.8, 'place': 0.8, 'fixed': 0.4, 'miles': 0.4, 'elvis': 0.8, 'prior': 0.8, 'coast': 1.0, 'green': 0.8, 'david': 0.6, 'lenny': 0.6, 'north': 0.8, 'never': 0.6, 'drama': 1.0, 'state': 1.0, 'space': 0.6, 'quite': 0.4, 'ivory': 0.4, 'panic': 0.8, 'among': 0.6, 'anish': 0.8, 'sites': 0.6, 'world': 0.4, 'staff': 1.0, 'their': 1.0, 'jared': 0.2, 'studs': 0.8, 'large': 0.8, 'steel': 0.8, 'press': 1.0, 'novel': 0.6, 'forum': 0.8, 'floor': 0.8, 'study': 0.6, 'heavy': 0.8, 'stars': 1.0, 'board': 0.8, 'house': 0.8, 'means': 0.8, 'acute': 0.8, 'total': 0.8, 'items': 1.0, 'koepp': 0.6, 'tends': 0.8, 'irish': 0.8, 'jodie': 0.6, 'light': 0.8, 'small': 1.0, 'still': 0.8, 'based': 0.4, 'tommy': 0.8, 'print': 0.8, 'waves': 0.4, 'track': 0.8, 'class': 1.0, 'those': 0.8, 'whose': 0.6, 'rooms': 0.8, 'might': 0.6, 'frame': 1.0, 'spain': 0.8, 'james': 0.6, 'areas': 1.0, 'known': 0.8, 'table': 1.0, 'often': 0.2}, '11-Down': {'environmentally': 0.6, 'recommendations': 0.67, 'representatives': 0.67, 'internationally': 0.67, 'straightforward': 0.33}, '12-Down': {'john': 0.5, 'role': 0.75, 'hunt': 1.0, 'team': 0.5, 'that': 0.75, 'days': 0.75, 'also': 1.0, 'soap': 0.75, 'grew': 0.5, 'cast': 1.0, 'chan': 0.75, 'whom': 0.25, 'anti': 0.75, 'hong': 1.0, 'span': 0.75, 'kyle': 0.5, 'best': 0.75, 'gary': 0.75, 'dawn': 0.75, 'such': 0.5, 'arts': 0.75, 'side': 1.0, 'took': 0.75, 'film': 0.25, 'quot': 0.5, 'rush': 0.75, 'near': 0.75, 'born': 0.75, 'lucy': 0.5, 'judy': 0.5, 'eric': 0.75, 'long': 1.0, 'clay': 0.5, 'qing': 0.75, 'most': 0.75, 'kong': 0.75, 'well': 0.5, 'back': 0.5, 'bell': 0.5, 'just': 0.75, 'noon': 0.75, 'kick': 0.5, 'owen': 0.5, 'king': 0.75, 'this': 0.75, 'both': 0.25, 'nice': 0.75, 'with': 0.25, 'jeff': 0.5, 'last': 1.0, 'lynn': 1.0, 'time': 0.75, 'east': 1.0, 'hour': 0.75, 'chon': 0.75, 'star': 0.75, 'list': 1.0, 'kung': 0.75, 'lisa': 1.0}, '13-Down': {'funk': 0.25, 'july': 0.25, 'week': 0.75, 'that': 0.75, 'post': 0.75, 'soul': 0.75, 'just': 0.25, 'boys': 0.75, 'song': 0.5, 'from': 0.5, 'span': 0.5, 'here': 0.5, 'drew': 0.75, 'quot': 0.25, 'hits': 0.5, 'each': 0.75, 'were': 0.5, 'part': 0.75, 'well': 0.5, 'oats': 0.75, 'home': 0.5, 'list': 0.75, 'with': 0.5, 'tops': 0.75, 'main': 0.75, 'help': 0.25, 'four': 0.5}, '15-Down': {}, '21-Down': {'obviously': 0.67, 'agnostics': 0.56, 'inscribed': 0.67, 'certainly': 0.78, 'referring': 0.89, 'watterson': 0.67, 'sentences': 0.67, 'attention': 0.78, 'something': 0.78, 'authority': 0.78, 'presorted': 0.78, 'sometimes': 0.78, 'statement': 0.67, 'jefferson': 0.78, 'otherwise': 0.78, 'laboratis': 0.78}, '26-Down': {'pdf': 0.67, 'see': 0.67, 'for': 0.33, 'new': 0.67, 'was': 0.33, 'bed': 0.67, 'the': 0.67, 'but': 0.33, 'ita': 0.67, 'dig': 0.0, 'ltd': 1.0, 'may': 0.0, 'and': 1.0, 'jan': 0.33}, '28-Down': {'hell': 1.0, 'move': 0.5, 'fuss': 0.75, 'jolt': 0.75, 'pain': 0.75, 'hurt': 0.75, 'ails': 1.0, 'cark': 0.5}, '30-Down': {'top': 0.33, 'rer': 0.67, 'far': 0.67, 'jim': 0.33, 'seo': 0.33, 'him': 0.67, 'one': 1.0, 'ww2': 0.0, 'the': 0.33, 'and': 1.0, 'mr.': 0.33, 'was': 0.67, 'fbi': 0.33, 'new': 0.33, 'for': 0.67, 'its': 1.0, 'men': 0.33, 'war': 0.67, 'now': 0.67, 'but': 0.33, 'six': 0.33}, '32-Down': {'john': 0.25, 'sued': 0.5, 'july': 0.0, 'that': 0.5, 'soul': 0.5, 'tune': 0.5, 'line': 0.5, 'epic': 0.25, 'slip': 0.5, 'pair': 0.5, 'jeff': 0.25, 'boys': 0.25, 'song': 0.5, 'loon': 0.25, 'cole': 0.5, 'into': 0.75, 'jimi': 0.5, 'late': 0.25, 'from': 0.5, 'weep': 0.5, 'span': 0.5, 'each': 0.25, 'real': 0.75, 'then': 0.5, 'tail': 0.5, 'such': 0.25, 'hiro': 0.75, 'beau': 0.75, 'away': 0.25, 'game': 0.5, 'this': 0.25, 'down': 0.25, 'film': 0.5, 'upon': 0.5, 'andy': 0.5, 'gary': 0.25, 'quot': 0.25, 'life': 0.5, 'omen': 0.5, 'will': 0.75, 'mark': 0.5, 'hear': 0.75, 'hero': 0.75, 'soft': 0.5, 'find': 0.75, 'wolf': 0.25, 'four': 0.5, 'most': 0.75, 'were': 0.75, 'wail': 0.5, 'jovi': 0.25, 'evil': 0.5, 'when': 0.5, 'male': 0.5, 'deep': 0.25, 'shed': 0.5, 'page': 0.5, 'teen': 0.75, 'tear': 0.75, 'rock': 0.5, 'only': 0.5, 'king': 0.5, 'bird': 0.75, 'pipe': 0.75, 'lane': 0.25, 'love': 0.25, 'with': 0.5, 'they': 0.25, 'moan': 0.5, 'howl': 0.5, 'baby': 0.25, 'sigh': 0.5, 'yell': 0.75, 'dogs': 0.0, 'over': 0.5}, '33-Down': {'bury': 0.25, 'that': 0.75, 'rome': 0.5, 'good': 0.25, 'also': 0.25, 'text': 0.5, 'came': 0.5, 'work': 0.25, 'salv': 0.25, 'west': 0.5, 'died': 0.5, 'from': 0.5, 'span': 0.75, 'mean': 0.75, 'this': 0.75, 'gaul': 0.0, 'butz': 0.0, 'pope': 0.5, 'june': 0.25, 'uses': 0.5, 'have': 0.25, 'list': 0.25, 'cede': 0.75, 'nero': 0.5, 'book': 0.0, 'bren': 0.25, 'with': 0.5, 'shoa': 0.5, 'time': 0.5, 'line': 0.5, 'able': 0.25, 'satz': 0.25, 'lyre': 0.75}, '34-Down': {'john': 0.25, 'know': 0.25, 'that': 1.0, 'matt': 1.0, 'much': 0.75, 'auer': 0.75, 'love': 0.5, 'song': 0.75, 'dave': 1.0, 'held': 0.0, 'some': 0.5, 'life': 0.5, 'from': 0.75, 'time': 0.5, 'span': 0.5, 'what': 0.75, 'this': 1.0, 'sing': 0.75, 'kind': 0.25, 'took': 0.25, 'film': 0.5, 'same': 0.75, 'band': 0.5, 'quot': 0.25, 'wild': 0.0, 'mark': 0.75, 'even': 0.5, 'drug': 0.75, 'name': 0.5, 'says': 0.75, 'ways': 0.5, 'levy': 0.5, 'well': 0.25, 'done': 0.75, 'does': 0.75, 'many': 1.0, 'they': 1.0, 'when': 0.5, 'stay': 1.0, 'form': 0.75, 'just': 0.5, 'meds': 0.75, 'term': 0.75, 'rock': 0.25, 'book': 0.0, 'home': 0.25, 'went': 0.5, 'neve': 0.5, 'with': 0.5, 'come': 0.5, 'four': 0.5}, '35-Down': {'pipe': 0.5, 'july': 0.5, 'city': 0.5, 'that': 0.5, 'home': 1.0, 'wood': 0.5, 'tuba': 0.5, 'alla': 1.0, 'when': 0.5, 'work': 0.5, 'rule': 1.0, 'span': 1.0, 'reed': 0.75, 'each': 1.0, 'oboe': 1.0, 'best': 0.5, 'days': 0.75, 'arts': 0.75, 'same': 1.0, 'solo': 1.0, 'upon': 1.0, 'quot': 0.75, 'used': 0.5, 'born': 0.75, 'even': 0.75, 'five': 0.5, 'york': 0.5, 'name': 1.0, 'four': 0.75, 'most': 0.5, 'left': 0.75, 'horn': 1.0, 'play': 0.5, 'hall': 1.0, 'well': 0.75, 'root': 1.0, 'have': 0.75, 'jazz': 0.25, 'they': 0.25, 'will': 0.75, 'harp': 0.75, 'just': 0.5, 'idrs': 0.75, 'term': 0.5, 'only': 0.5, 'give': 0.5, 'with': 0.75, 'been': 0.75, 'over': 0.75}, '37-Down': {'born': 0.5, 'city': 0.25, 'that': 0.5, 'edge': 0.5, 'song': 0.5, 'also': 0.5, 'mail': 0.25, 'slow': 0.5, 'mayo': 0.0, 'nagi': 0.25, 'from': 0.5, 'span': 0.75, 'jaws': 0.5, 'weft': 0.25, 'warp': 0.5, 'such': 0.5, 'used': 0.5, 'film': 0.5, 'quot': 0.25, 'band': 0.25, 'claw': 0.25, 'york': 0.5, 'type': 0.25, 'name': 0.25, 'chop': 0.5, 'coup': 0.25, 'were': 0.25, 'cuts': 0.5, 'have': 0.5, 'they': 0.5, 'noda': 0.0, 'burn': 0.5, 'lift': 0.25, 'saws': 0.75, 'jake': 0.0, 'work': 0.5, 'comb': 0.25, 'snag': 0.5, 'bird': 0.25, 'both': 0.25, 'list': 0.25, 'nike': 0.0, 'with': 0.25, 'kite': 0.25, 'time': 0.5, 'been': 0.5, 'isbn': 0.5, 'trim': 0.5}, '40-Down': {'its': 1.0, 'one': 1.0, 'his': 1.0, 'gps': 0.33, 'lug': 1.0, 'the': 1.0, 'and': 0.67, 'arc': 1.0, 'col': 0.33, 'are': 1.0, 'all': 1.0, 'was': 0.67, 'led': 0.67, 'day': 0.33, 'kit': 0.67, 'lcd': 0.67, 'new': 0.67, 'for': 0.67, 'sea': 0.67, 'pre': 1.0, 'two': 1.0, 'amp': 0.33, 'old': 0.67, 'use': 1.0, 'esh': 0.67, 'who': 0.67, 'saw': 0.33, 'usb': 0.67}, '43-Down': {'gia': 0.67, 'are': 0.67, 'was': 1.0, 'but': 0.33, 'the': 1.0, 'bel': 0.67, 'one': 0.33, 'see': 1.0, 'vua': 0.33, 'pdo': 0.33, 'bra': 0.33, 'opp': 0.33, 'and': 0.67, 'has': 1.0}, '45-Down': {'top': 0.67, 'dad': 0.67, 'for': 0.67, 'dog': 0.67, 'set': 0.67, 'old': 0.67, 'can': 1.0, 'our': 0.67, 'one': 0.67, 'yul': 0.33, 'fun': 0.33, 'rod': 1.0, 'the': 0.67, 'and': 0.67, 'any': 0.67, 'toy': 1.0, 'are': 0.67, 'pop': 0.33, 'you': 0.33, 'was': 0.67, 'big': 0.67, 'man': 0.67, 'pet': 0.33, 'two': 0.67, 'kid': 0.67, 'may': 0.67, 'has': 1.0, 'cgi': 0.67, 'boy': 0.67, 'hee': 0.67, 'abc': 0.33, 'sit': 1.0, 'see': 0.67}, '50-Down': {'plant': 1.0, 'miles': 0.4, 'named': 0.4, 'vodka': 0.6, 'adams': 0.8, 'idaho': 0.8, 'water': 0.6, 'china': 0.6, 'share': 0.8, 'world': 0.4, 'their': 0.6, 'syria': 0.8, 'after': 0.6, 'lager': 0.8, 'booze': 0.4, 'beers': 0.6, 'irish': 1.0, 'draft': 0.8, 'third': 0.6, 'drink': 1.0, 'spelt': 1.0, 'class': 0.6, 'while': 0.6, 'areas': 0.8}, '52-Down': {'plant': 1.0, 'first': 0.8, 'about': 0.4, 'atoms': 0.8, 'thing': 0.4, 'decay': 0.8, 'state': 0.6, 'would': 0.8, 'phase': 0.6, 'stuff': 1.0, 'split': 0.6, 'which': 0.6, 'could': 0.8, 'there': 0.4, 'equal': 0.8, 'cells': 0.8, 'fermi': 0.8, 'clean': 0.8, 'based': 1.0, 'power': 1.0, 'class': 0.8, 'italy': 0.8, 'ridge': 0.6, 'apart': 0.8}, '54-Down': {}, '55-Down': {'ghost': 0.6, 'first': 0.8, 'bring': 0.4, 'label': 0.4, 'henry': 1.0, 'since': 0.8, 'topic': 0.4, 'story': 0.8, 'green': 0.4, 'carol': 0.6, 'seuss': 0.6, 'claus': 0.6, 'atoll': 0.2, 'trees': 0.6, 'buble': 0.2, 'quiet': 0.6, 'party': 1.0, 'fifth': 0.4, 'prose': 0.6, 'homes': 0.8, 'other': 0.6, 'peter': 0.8, 'stole': 0.6, 'early': 0.8, 'which': 0.4, 'hours': 0.6, 'saint': 1.0, 'where': 0.8, 'spike': 1.0, 'ocean': 0.2, 'world': 0.4, 'light': 0.8, 'lamps': 0.4, 'gifts': 0.4, 'based': 0.6, 'royal': 0.6, 'coral': 0.6, 'kinds': 0.6, 'price': 0.6, 'shows': 0.4, 'being': 0.6, 'class': 0.8, 'major': 0.4, 'bulbs': 0.2, 'santa': 0.8, 'under': 0.6, 'album': 0.2, 'known': 0.2, 'stage': 1.0}, '56-Down': {'vast': 1.0, 'bats': 0.75, 'food': 0.75, 'tale': 0.75, 'that': 0.75, 'race': 0.75, 'line': 0.5, 'also': 0.25, 'evil': 0.5, 'into': 0.5, 'rule': 0.5, 'some': 0.75, 'from': 1.0, 'hong': 0.75, 'moon': 1.0, 'span': 1.0, 'each': 0.5, 'ever': 0.75, 'yuen': 0.75, 'such': 0.5, 'ying': 0.5, 'kind': 0.5, 'film': 0.75, 'biao': 0.5, 'been': 0.75, 'week': 0.5, 'will': 0.5, 'five': 0.25, 'make': 0.5, 'once': 0.75, 'part': 1.0, 'hung': 0.5, 'have': 0.5, 'bite': 0.25, 'they': 0.5, 'dead': 0.5, 'weak': 0.5, 'form': 1.0, 'nina': 0.25, 'made': 0.75, 'book': 0.75, 'lead': 0.5, 'pour': 0.75, 'tour': 0.75, 'this': 0.75, 'cozy': 0.5, 'love': 0.5, 'kill': 0.5, 'what': 0.75, 'with': 0.25, 'kong': 0.75, 'wolf': 1.0, 'them': 0.75}, '57-Down': {'girl': 0.75, 'good': 0.5, 'todd': 0.5, 'jack': 0.25, 'than': 0.75, 'from': 0.5, 'head': 0.75, 'span': 0.25, 'kirk': 0.5, 'hymn': 0.75, 'fife': 0.5, 'aunt': 0.75, 'such': 0.5, 'film': 0.5, 'quot': 0.5, 'andy': 0.5, 'tags': 0.5, 'clan': 0.5, 'five': 0.75, 'have': 1.0, 'boys': 0.25, 'spin': 0.25, 'hope': 1.0, 'pooh': 1.0, 'slow': 0.5, 'burt': 0.5, 'made': 0.75, 'show': 0.5, 'dale': 0.75, 'list': 0.75, 'with': 0.75, 'main': 0.75, 'opie': 0.75, 'life': 0.75}, '61-Down': {'see': 1.0, 'few': 0.33, 'are': 1.0, 'all': 1.0, 'for': 0.33, 'not': 1.0, 'non': 1.0, 'ads': 0.67, 'the': 1.0, 'psd': 0.33, 'can': 1.0, 'was': 0.33, 'etc': 1.0, 'web': 0.33, 'one': 0.67, 'how': 0.33, 'and': 0.67}, '62-Down': {'pie': 1.0, 'ice': 0.67, 'jif': 0.33, 'the': 0.67, 'put': 0.67, 'and': 0.67, 'tad': 0.67, 'was': 0.33, 'too': 1.0, 'bar': 0.67, 'say': 0.67, 'bit': 0.67, 'she': 0.67, 'egg': 0.67, 'cup': 0.33, 'amp': 0.33, 'npr': 0.67, 'but': 0.33}, '64-Down': {'upa': 0.33, 'end': 1.0, 'for': 0.67, 'set': 1.0, 'ago': 0.67, 'due': 0.67, 'one': 1.0, 'sky': 0.67, 'ice': 1.0, 'the': 1.0, 'and': 1.0, 'air': 1.0, 'jay': 0.67, 'war': 0.67, 'sun': 0.67, 'are': 1.0, 'all': 0.67, 'big': 0.67, 'was': 0.67, 'too': 1.0, 'new': 1.0, 'jet': 0.67, 'way': 0.67, 'its': 1.0, 'two': 0.67, 'toy': 1.0, 'may': 1.0, 'fly': 0.33, 'amp': 0.67, 'has': 0.67, 'did': 1.0, 'off': 0.67, 'but': 0.67, 'bus': 0.67, 'see': 1.0, 'via': 0.67}}\n"
     ]
    }
   ],
   "source": [
    "coordslookup = getCoordsLookup(puzzle)\n",
    "# for k,v in coordslookup.items():\n",
    "#     print(k)\n",
    "#     print(pd.DataFrame(v).T)\n",
    "intersections = getIntersections(puzzle)\n",
    "print(intersections)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
